// ğŸ¤– INTÃ‰GRATION OLLAMA - IA Locale pour NextStep

// Configuration Ollama
const OLLAMA_CONFIG = {
    baseURL: 'http://localhost:11434',
    model: 'llama2', // ou 'mistral', 'codellama', etc.
    timeout: 30000
};

// Fonction pour vÃ©rifier si Ollama est disponible
async function checkOllamaAvailability() {
    try {
        const response = await fetch(`${OLLAMA_CONFIG.baseURL}/api/tags`, {
            method: 'GET',
            headers: { 'Content-Type': 'application/json' }
        });

        if (response.ok) {
            const data = await response.json();
            console.log("âœ… Ollama disponible avec les modÃ¨les:", data.models?.map(m => m.name).join(', '));
            return true;
        }
    } catch (error) {
        console.warn("âš ï¸ Ollama non disponible:", error.message);
    }
    return false;
}

// Fonction pour analyser un CV avec Ollama
async function analyzeCVWithOllama(cvText) {
    console.log("ğŸ¤– Analyse du CV avec Ollama...");

    const prompt = `Tu es un expert en recrutement. Analyse ce CV et extrait les informations suivantes au format JSON strict :
{
  "name": "Nom complet",
  "role": "IntitulÃ© de poste recherchÃ©",
  "skills": ["compÃ©tence1", "compÃ©tence2", "compÃ©tence3"],
  "location": "Ville",
  "summary": "RÃ©sumÃ© en une phrase"
}

CV:
${cvText.substring(0, 2000)}

RÃ©ponds UNIQUEMENT avec le JSON, sans texte avant ou aprÃ¨s.`;

    try {
        const response = await fetch(`${OLLAMA_CONFIG.baseURL}/api/generate`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                model: OLLAMA_CONFIG.model,
                prompt: prompt,
                stream: false,
                options: {
                    temperature: 0.3,
                    top_p: 0.9
                }
            })
        });

        if (!response.ok) {
            throw new Error(`Ollama API error: ${response.status}`);
        }

        const data = await response.json();
        const responseText = data.response || '';

        console.log("ğŸ“„ RÃ©ponse brute Ollama:", responseText);

        // Extraire le JSON de la rÃ©ponse
        const jsonMatch = responseText.match(/\{[\s\S]*\}/);
        if (jsonMatch) {
            const parsed = JSON.parse(jsonMatch[0]);
            console.log("âœ… CV analysÃ© avec Ollama:", parsed);
            return parsed;
        } else {
            throw new Error("Pas de JSON trouvÃ© dans la rÃ©ponse");
        }

    } catch (error) {
        console.error("âŒ Erreur Ollama:", error);
        return null;
    }
}

// Fonction pour le chatbot avec Ollama
async function chatWithOllama(userMessage, context = []) {
    console.log("ğŸ’¬ Question utilisateur:", userMessage);

    const systemPrompt = `Tu es NextStep AI, un assistant spÃ©cialisÃ© dans l'orientation professionnelle et la recherche d'alternance. 
Tu connais les rÃ©fÃ©rentiels ROME et RNCP. Tu aides les Ã©tudiants Ã  trouver des offres d'alternance compatibles avec leur formation.
RÃ©ponds de maniÃ¨re concise et professionnelle en franÃ§ais.`;

    const prompt = `${systemPrompt}

Conversation prÃ©cÃ©dente:
${context.map(msg => `${msg.role}: ${msg.content}`).join('\n')}

Utilisateur: ${userMessage}
