# ü§ñ GUIDE OLLAMA - IA Locale pour NextStep

## üéØ QU'EST-CE QU'OLLAMA ?

Ollama est une IA qui tourne **localement sur votre ordinateur**. Pas besoin de cl√© API, pas de limite d'appels !

---

## ‚úÖ INSTALLATION (5 minutes)

### √âtape 1 : T√©l√©charger Ollama

Aller sur : **https://ollama.ai**

T√©l√©charger la version Mac et installer.

### √âtape 2 : T√©l√©charger un mod√®le

Ouvrir un terminal et taper :

```bash
ollama pull llama2
```

Ou pour un mod√®le plus rapide :

```bash
ollama pull mistral
```

### √âtape 3 : V√©rifier que √ßa marche

```bash
ollama list
```

Vous devriez voir :
```
NAME            SIZE
llama2:latest   3.8GB
```

---

## üîß INT√âGRATION DANS NEXTSTEP

### √âtape 1 : Ajouter le script dans index.html

Ouvrir `index.html` et ajouter apr√®s la ligne 20 :

```html
<script src="ollama_integration_complete.js"></script>
```

### √âtape 2 : Modifier script.js

Remplacer la fonction `analyzeCVWithHF` (ligne 1779) par :

```javascript
async function analyzeCVWithHF(text) {
    // Essayer Ollama d'abord
    if (await checkOllamaAvailability()) {
        const result = await analyzeCVWithOllama(text);
        if (result) return result;
    }
    
    // Fallback sur donn√©es de d√©mo
    return {
        name: "Profil √âtudiant",
        role: "D√©veloppeur Full Stack",
        skills: ["JavaScript", "React", "Node.js"],
        summary: "√âtudiant passionn√© par le d√©veloppement web"
    };
}
```

---

## üß™ TESTER

### 1. Lancer Ollama

Ollama se lance automatiquement apr√®s l'installation. Sinon :

```bash
ollama serve
```

### 2. Lancer le site

```bash
cd /Users/marwan/.gemini/antigravity/playground/exo-magnetar
python3 -m http.server 8000
```

Ouvrir : `http://localhost:8000/index.html`

### 3. Uploader un CV

Dans la console (F12), vous devriez voir :

```
‚úÖ Ollama disponible
ü§ñ Analyse du CV avec Ollama...
‚úÖ CV analys√© avec Ollama: {name: "...", role: "...", ...}
```

---

## üéì POUR LE HACKATHON

### Avantages d'Ollama :

1. ‚úÖ **Gratuit** - Pas de cl√© API
2. ‚úÖ **Illimit√©** - Pas de limite d'appels
3. ‚úÖ **Rapide** - Tourne en local
4. ‚úÖ **Priv√©** - Les donn√©es ne sortent pas de votre machine
5. ‚úÖ **Offline** - Fonctionne sans Internet

### Phrase pour le jury :

> "Nous utilisons Ollama, une IA locale qui tourne directement sur la machine. Cela garantit la confidentialit√© des CV et permet un nombre illimit√© d'analyses sans co√ªt."

---

## üìä MOD√àLES RECOMMAND√âS

| Mod√®le | Taille | Vitesse | Qualit√© |
|--------|--------|---------|---------|
| `llama2` | 3.8 GB | Moyen | Bonne |
| `mistral` | 4.1 GB | Rapide | Excellente |
| `codellama` | 3.8 GB | Moyen | Bonne (code) |

Pour changer de mod√®le, modifier `ollama_integration_complete.js` ligne 5 :

```javascript
model: 'mistral' // au lieu de 'llama2'
```

---

## üîç D√âPANNAGE

### Probl√®me : "Ollama non disponible"

**Solution** : V√©rifier qu'Ollama tourne :

```bash
curl http://localhost:11434/api/tags
```

Si erreur, lancer :

```bash
ollama serve
```

### Probl√®me : "Mod√®le non trouv√©"

**Solution** : T√©l√©charger le mod√®le :

```bash
ollama pull llama2
```

---

**Ollama est pr√™t ! T√©l√©chargez-le et testez ! üöÄ**
